#2025/04/07
import streamlit as st
import pandas as pd
import json
import re
from gpt_api import ask_gpt
from prompt_utils import make_category_extraction_prompt

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
scores_df = pd.read_csv("./input/cafe_category_scores.csv")
reviews_df = pd.read_csv("./input/cafe_reviews_analysis_v3_part_full.csv")

valid_categories = scores_df.columns[1:].tolist()

category_mapping = {
    # æ˜ç¢ºãªå¤‰æ›
    "é™ã‹ã•": "é›°å›²æ°—", "ã‚¢ã‚¯ã‚»ã‚¹": "ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³", "ãŠã—ã‚ƒã‚Œã•": "é›°å›²æ°—",
    "ãã‚Œã„ã•": "æ¸…æ½”æ„Ÿ", "ç©ºã„ã¦ã‚‹": "æ··é›‘", "ç©ºé–“": "åº§å¸­",

    # å£ã‚³ãƒŸå®Ÿãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šæ‹¡å¼µ
    "å‘³": "ã”ã¯ã‚“",
    "é£Ÿäº‹": "ã”ã¯ã‚“",
    "æ–™ç†": "ã”ã¯ã‚“",
    "å“è³ª": "ã”ã¯ã‚“",
    "æ„Ÿæƒ…": "ãã®ä»–",
    "æ„Ÿæƒ³": "ãã®ä»–",
    "æº€è¶³åº¦": "ãã®ä»–",
    "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹": "ã‚µãƒ¼ãƒ“ã‚¹",
    "æ¼”å‡º": "é›°å›²æ°—",
    "ãƒ‡ã‚¶ã‚¤ãƒ³": "é›°å›²æ°—",
    "ã‚¤ãƒ³ãƒ†ãƒªã‚¢": "é›°å›²æ°—",
    "ç’°å¢ƒ": "é›°å›²æ°—",
    "è¦‹ãŸç›®": "é›°å›²æ°—",
    "åº—": "ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
    "åº—èˆ—": "ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
    "ãƒ¡ãƒ‹ãƒ¥ãƒ¼": "ãã®ä»–",
    "å•†å“": "ãã®ä»–",
    "å“æƒãˆ": "ãã®ä»–",
    "è©•åˆ¤": "å®¢",
    "å¿ƒç†çŠ¶æ…‹": "ãã®ä»–",
    "ä¾¿åˆ©ã•": "ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
    "ã‚¤ãƒ™ãƒ³ãƒˆ": "ã‚µãƒ¼ãƒ“ã‚¹",
    "ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º": "ã‚µãƒ¼ãƒ“ã‚¹",
    "èŒ¶": "ç´…èŒ¶",
    "ã‚½ãƒ¼ã‚¹": "ã”ã¯ã‚“",
    "ã‚¹ãƒ¼ãƒ—": "ã”ã¯ã‚“"
}


# --- Streamlit åˆæœŸåŒ– ---
st.set_page_config(page_title="ã‚«ãƒ•ã‚§æ¨è–¦ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", page_icon="â˜•")
st.title("â˜• ã‚«ãƒ•ã‚§æ¨è–¦ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")

if "step" not in st.session_state:
    st.session_state.step = 1
    st.session_state.weights = {}
    st.session_state.history = []

def extract_categories(user_text):
    prompt = make_category_extraction_prompt(user_text)
    category_weights = ask_gpt(prompt)
    st.write("ğŸ” GPTã®è¿”ç­”:", category_weights)
    json_text = re.search(r"\{.*\}", category_weights, re.DOTALL)
    if not json_text:
        raise ValueError("JSONå½¢å¼ã®å‡ºåŠ›ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    raw_weights = json.loads(json_text.group())
    mapped = {}
    for k, v in raw_weights.items():
        mapped_key = category_mapping.get(k, k)
        if mapped_key in valid_categories:
            mapped[mapped_key] = v
    return mapped

# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚¹ãƒ†ãƒƒãƒ— ---
if st.session_state.step == 1:
    user_input = st.text_input("ã©ã‚“ãªã‚«ãƒ•ã‚§ãŒç†æƒ³ã§ã™ã‹ï¼Ÿï¼ˆä¾‹ï¼šé§…ã‹ã‚‰è¿‘ãã¦ã‚¹ã‚¤ãƒ¼ãƒ„ãŒç¾å‘³ã—ã„ã‚«ãƒ•ã‚§ï¼‰")
    if user_input:
        try:
            extracted = extract_categories(user_input)
            st.session_state.weights.update(extracted)
            st.session_state.history.append(user_input)
            st.session_state.step += 1
            st.rerun()
        except Exception as e:
            st.error("ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
            st.write("ã‚¨ãƒ©ãƒ¼å†…å®¹:", e)

elif st.session_state.step == 2:
    followup = st.text_input("ä»–ã«æ°—ã«ãªã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿï¼ˆä¾‹ï¼šé™ã‹ãªå ´æ‰€ãŒã„ã„ã€ä¾¡æ ¼ã‚‚æ°—ã«ãªã‚‹ï¼‰")
    if followup:
        try:
            extracted = extract_categories(followup)
            st.session_state.weights.update(extracted)
            st.session_state.history.append(followup)
            st.session_state.step += 1
            st.rerun()
        except Exception as e:
            st.error("è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
            st.write("ã‚¨ãƒ©ãƒ¼å†…å®¹:", e)

elif st.session_state.step == 3:
    final = st.text_input("ã•ã‚‰ã«é‡è¦–ã—ãŸã„ã“ã¨ãŒã‚ã‚Œã°æ•™ãˆã¦ãã ã•ã„ã€‚ï¼ˆä¾‹ï¼šæ¸…æ½”æ„ŸãŒã‚ã‚‹ã¨å¬‰ã—ã„ï¼‰")
    if final:
        try:
            extracted = extract_categories(final)
            st.session_state.weights.update(extracted)
            st.session_state.history.append(final)
            st.session_state.step += 1
            st.rerun()
        except Exception as e:
            st.error("è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
            st.write("ã‚¨ãƒ©ãƒ¼å†…å®¹:", e)

# --- æœ€çµ‚æ¨è–¦ ---
elif st.session_state.step == 4:
    weights = st.session_state.weights

    def weighted_score(row):
        return sum(
            row[c] * weights[c] for c in weights if c in row and not pd.isna(row[c])
        )

    scores_df["score"] = scores_df.apply(weighted_score, axis=1)
    top4 = scores_df.sort_values("score", ascending=False).head(4)

    st.success("âœ… ã‚ãªãŸã®å¸Œæœ›ã«åˆã†ã‚«ãƒ•ã‚§ã‚’ææ¡ˆã—ã¾ã™ï¼")

    for i in range(4):
        row = top4.iloc[i]
        cafe_name = row["ã‚«ãƒ•ã‚§å"]
        st.subheader(f"â˜• {cafe_name}ï¼ˆã‚¹ã‚³ã‚¢: {row['score']:.2f}ï¼‰")

        # ã‚¹ã‚³ã‚¢ä¸Šä½ã‚«ãƒ†ã‚´ãƒªã‚’è¡¨ç¤ºï¼ˆä¸Šä½5ã‚«ãƒ†ã‚´ãƒªï¼‰
        top_categories = (
            row[weights.keys()]
            .sort_values(ascending=False)
            .dropna()
            .head(5)
        )
        st.markdown("ğŸ“Š **ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚¹ã‚³ã‚¢**")
        st.dataframe(top_categories.rename("ã‚¹ã‚³ã‚¢"))

        # é–¢é€£å£ã‚³ãƒŸã®è¡¨ç¤ºï¼ˆé‡è¤‡é™¤å»ã‚ã‚Šï¼‰
        st.markdown("ğŸ—£ **å®Ÿéš›ã®å£ã‚³ãƒŸ**")
        related_reviews = reviews_df[
            (reviews_df["ã‚«ãƒ•ã‚§å"] == cafe_name) &
            (reviews_df["ã‚«ãƒ†ã‚´ãƒª"].isin(weights.keys())) &
            (reviews_df["ã‚¹ã‚³ã‚¢"] >= 0.5)
        ].dropna().drop_duplicates(subset="å£ã‚³ãƒŸ").sort_values("ã‚¹ã‚³ã‚¢", ascending=False).head(3)

        if related_reviews.empty:
            st.write("å£ã‚³ãƒŸãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            for _, review_row in related_reviews.iterrows():
                cat = review_row["ã‚«ãƒ†ã‚´ãƒª"]
                txt = review_row["å£ã‚³ãƒŸ"]
                st.markdown(f"- ğŸ’¬ **{cat}**: {txt}")


    if st.button("ğŸ”„ æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™"):
        st.session_state.clear()
        st.rerun()
